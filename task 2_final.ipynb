{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13944918,"sourceType":"datasetVersion","datasetId":8885317}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U mne==1.10.1 mne-bids==0.17.0 --quiet\n!pip install -U eegdash==0.3.8 s3fs==2025.9.0 fsspec==2025.9.0 pandas==2.3.3 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T07:10:04.142726Z","iopub.execute_input":"2025-12-02T07:10:04.143623Z","iopub.status.idle":"2025-12-02T07:11:36.928864Z","shell.execute_reply.started":"2025-12-02T07:10:04.143588Z","shell.execute_reply":"2025-12-02T07:11:36.928107Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/71.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport os, math, random\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nimport warnings\nimport mne\nfrom mne_bids import BIDSPath, read_raw_bids\nmne.set_log_level('ERROR')\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*Unable to map the following column\\\\(s\\\\) to to MNE.*\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T07:13:25.374055Z","iopub.execute_input":"2025-12-02T07:13:25.374769Z","iopub.status.idle":"2025-12-02T07:13:25.379500Z","shell.execute_reply.started":"2025-12-02T07:13:25.374743Z","shell.execute_reply":"2025-12-02T07:13:25.378809Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"SFREQ = 100  # Sampling rate (Hz)\nWIN_SEC = 4  # Window size (seconds)\nCROP_SEC = 2  # Random crop size (seconds)\nSTRIDE_SEC = 2  # Window stride (seconds)\nTASK = \"contrastChangeDetection\"  # Primary task\n\n# Path to the parent folder containing R6_L100_bdf and R5_L100_bdf\nDATA_ROOT = Path(\"/kaggle/input/eeg-dataset\")\n\nTRAIN_RELEASES = [\"R6_L100_bdf\"]\nVAL_RELEASE = \"R5_L100_bdf\"\n\nSUB_RM = [\"NDARAC350XUM\", \"NDARAJ689BVN\"] \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef seed_all(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_all(42)\n\ndef resolve_double_nested_path(data_root, release):\n    outer = data_root / release\n    inner = outer / release\n    if (inner.exists() and (inner / \"participants.tsv\").exists()):\n        return inner\n    elif (outer.exists() and (outer / \"participants.tsv\").exists()):\n        return outer\n    else:\n        return None\n\ndef load_participants_data(release_path):\n    participants_file = release_path / \"participants.tsv\"\n    if not participants_file.exists():\n        return pd.DataFrame()\n    df = pd.read_csv(participants_file, sep='\\t')\n    return df\n\n# Helper to load RAW data for a subject/run (Used by the cache and the dataset)\ndef load_raw_eeg(subject_path, subject_id, task, run=None):\n    \"\"\"Load a single EEG file using MNE-BIDS. Returns mne.io.Raw object.\"\"\"\n    try:\n        # BIDS_ROOT is subject_path.parent\n        bids_path = BIDSPath(\n            subject=subject_id,\n            task=task,\n            run=run,\n            datatype='eeg',\n            extension='.bdf',\n            root=subject_path.parent\n        )\n        raw = read_raw_bids(bids_path, verbose=False)\n        raw.load_data()\n        \n        # Validation checks\n        if raw.n_times < 4 * SFREQ or len(raw.ch_names) != 129:\n             return None\n             \n        return raw\n    except Exception:\n        return None\n\ndef load_release_data_lazy(release, task=TASK, data_root=DATA_ROOT):\n    \"\"\"\n    Scans the data and returns a list of window pointers (metadata), \n    not the actual EEG data, to avoid filling RAM.\n    \"\"\"\n    release_path = resolve_double_nested_path(data_root, release)\n\n    if not release_path.exists():\n        print(f\"Warning: Release path {release_path} not found\")\n        return []\n\n    participants_df = load_participants_data(release_path)\n\n    if participants_df.empty:\n        print(f\"No participants data found for {release}\")\n        return []\n\n    window_pointers = []\n    win_samples = int(WIN_SEC * SFREQ)\n    stride_samples = int(STRIDE_SEC * SFREQ)\n    \n    # Store the BIDS root for easier access later\n    bids_root = str(release_path)\n\n    for _, row in tqdm(\n        participants_df.iterrows(),\n        total=len(participants_df),\n        desc=f\"Scanning {release} for windows\"\n    ):\n        subject_id = row['participant_id'].replace('sub-', '')\n\n        if subject_id in SUB_RM:\n            continue\n\n        # Extract demographics and label\n        age = row.get('age', np.nan)\n        sex = row.get('sex', np.nan)\n        handedness = row.get('handedness', np.nan)\n        externalizing = row.get('externalizing', np.nan)\n        \n        try:\n            externalizing = float(externalizing)\n            if not math.isfinite(externalizing):\n                continue\n        except Exception:\n            continue\n            \n        sex_str = str(sex).strip().lower()\n        if sex_str in ['female', 'f', '2']:\n            sex_encoded = 1.0\n        elif sex_str in ['male', 'm', '1']:\\\n            sex_encoded = 0.0\n        else:\n            sex_encoded = np.nan\n        \n        # Path to subject directory inside the resolved release folder\n        subject_path = release_path / f\"sub-{subject_id}\"\n\n        runs = [1, 2, 3] if task == \"contrastChangeDetection\" else [None]\n\n        for run in runs:\n            try:\n               \n                bids_path_check = BIDSPath(\n                    subject=subject_id, task=task, run=run, datatype='eeg', extension='.bdf', root=release_path\n                )\n                if not bids_path_check.fpath.exists():\n                    continue\n\n                raw = load_raw_eeg(subject_path, subject_id, task, run)\n                if raw is None:\n                    continue\n\n                n_times = raw.n_times\n                del raw # IMMEDIATELY RELEASE MEMORY\n\n                starts = range(0, n_times - win_samples + 1, stride_samples)\n                \n                # Store pointers\n                for start in starts:\n                    window_pointers.append({\n                        \"bids_root\": bids_root,\n                        \"subject\": subject_id,\n                        \"task\": task,\n                        \"run\": run,\n                        \"start_sample\": start,\n                        \"end_sample\": start + win_samples,\n                        \"age\": age,\n                        \"sex\": sex_encoded,\n                        \"handedness\": handedness,\n                        \"externalizing\": externalizing,\n                    })\n\n            except Exception:\n                continue\n\n    print(f\"Scanned and generated {len(window_pointers)} window pointers from {release}\")\n    return window_pointers\n\nclass EEGWindowsDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset that lazily loads EEG data from disk using MNE-BIDS path metadata,\n    and uses a simple LRU cache to keep the last few raw files in memory.\n    \"\"\"\n    def __init__(self, data_list, crop_samples, keep_idx, seed=42, cache_size=3):\n        self.data_list = data_list\n        self.crop_samples = crop_samples\n        self.keep_idx = keep_idx\n        self.rng = random.Random(seed)\n        \n        # Simple cache for raw objects to speed up repeated access to the same file\n        self.raw_cache = {} \n        self.cache_keys = []\n        self.cache_size = cache_size\n        \n        # For memory efficiency, pre-calculate path to subject folder\n        self._subject_paths = {}\n        for item in data_list:\n            key = (item['subject'], item['run'])\n            if key not in self._subject_paths:\n                bids_root = Path(item['bids_root'])\n                self._subject_paths[key] = bids_root / f\"sub-{item['subject']}\"\n    \n    def __len__(self):\n        return len(self.data_list)\n\n    def _get_raw_from_cache(self, subject_id, run):\n        \"\"\"Fetches raw object from cache or loads it from disk.\"\"\"\n        key = (subject_id, run)\n        \n        # 1. Hit: Move key to front (MRU)\n        if key in self.raw_cache:\n            self.cache_keys.remove(key)\n            self.cache_keys.append(key)\n            return self.raw_cache[key]\n        \n        # 2. Miss: Load from disk\n        item = next(item for item in self.data_list if (item['subject'], item['run']) == key)\n        subject_path = self._subject_paths[key]\n\n        raw = load_raw_eeg(subject_path, subject_id, item['task'], run)\n        \n        if raw is None:\n            raise FileNotFoundError(f\"Could not load raw file for {subject_id}, run {run}\")\n            \n        # 3. Add to cache (LRU eviction)\n        if len(self.raw_cache) >= self.cache_size:\n            lru_key = self.cache_keys.pop(0) # Pop oldest (LRU)\n            del self.raw_cache[lru_key]\n        \n        self.raw_cache[key] = raw\n        self.cache_keys.append(key)\n        \n        return raw\n\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        \n        raw = self._get_raw_from_cache(item['subject'], item['run'])\n        \n        # Extract the window using MNE method for safe memory access\n        eeg, _ = raw[:, item['start_sample']:item['end_sample']]\n        # MNE returns (n_channels, n_times)\n        \n        # Take first 128 channels if 129 present\n        if eeg.shape[0] == 129:\n            eeg = eeg[:128, :]\n        \n        # Convert to tensor and apply crop/normalization (rest of your original logic)\n        eeg = torch.from_numpy(eeg.copy()).float()\n        C, T = eeg.shape\n        \n        # Random crop\n        if T < self.crop_samples:\n            pad_amount = self.crop_samples - T\n            eeg = torch.nn.functional.pad(eeg, (0, pad_amount), mode='constant', value=0)\n            start_crop = 0\n            stop_crop = self.crop_samples\n        else:\n            start_crop = self.rng.randint(0, T - self.crop_samples)\n            stop_crop = start_crop + self.crop_samples\n            eeg = eeg[:, start_crop:stop_crop]\n        \n        # Per-window z-score normalization\n        mu = eeg.mean(dim=1, keepdim=True)\n        sd = eeg.std(dim=1, keepdim=True)\n        eeg = (eeg - mu) / (sd + 1e-6)\n        eeg = torch.nan_to_num(eeg, nan=0.0, posinf=0.0, neginf=0.0)\n        eeg = torch.clamp(eeg, min=-1e3, max=1e3)\n        \n        # Get label and demographics (mostly copied from your original code)\n        y = torch.tensor([item['externalizing']], dtype=torch.float32)\n        \n        age = item['age']\n        try:\n            age = float(age) if age is not None and math.isfinite(float(age)) else np.nan\n        except:\n            age = np.nan\n        \n        sex = item['sex']\n        hand = item['handedness']\n        try:\n            hand = float(hand) if hand is not None and math.isfinite(float(hand)) else np.nan\n        except:\n            hand = np.nan\n        \n        full_demo = np.array([age, sex, hand], dtype=np.float32)\n        if len(self.keep_idx) > 0:\n            demo = torch.from_numpy(full_demo[self.keep_idx])\n        else:\n            demo = torch.empty(0, dtype=torch.float32)\n        \n        info = {\n            'subject': item['subject'],\n            'task': item['task'],\n            'run': item['run'],\n        }\n        crop_idx = (item['start_sample'] + start_crop, item['start_sample'] + stop_crop)\n        \n        return eeg, y, demo, crop_idx, info\n\n\ndef extract_unique_demographics(data_list):\n    seen = {}\n    for item in data_list:\n        sid = item['subject']\n        if sid in seen:\n            continue\n        \n        age = item['age']\n        try:\n            age = float(age) if age is not None and math.isfinite(float(age)) else np.nan\n        except:\n            age = np.nan\n        \n        sex = item['sex']\n        hand = item['handedness']\n        try:\n            hand = float(hand) if hand is not None and math.isfinite(float(hand)) else np.nan\n        except:\n            hand = np.nan\n        \n        seen[sid] = [age, sex, hand]\n    \n    arr = np.array(list(seen.values()), dtype=np.float32) if seen else np.zeros((0, 3), dtype=np.float32)\n    return arr\n\nclass SafeStandardScaler(StandardScaler):\n    def fit(self, X, y=None):\n        super().fit(X, y)\n        if hasattr(self, \"scale_\"):\n            bad = ~np.isfinite(self.scale_) | (self.scale_ == 0)\n            self.scale_[bad] = 1.0\n        if hasattr(self, \"var_\"):\n            self.var_[~np.isfinite(self.var_)] = 0.0\n        if hasattr(self, \"mean_\"):\n            self.mean_[~np.isfinite(self.mean_)] = 0.0\n        return self\n\ndef build_demo_transform(train_data):\n    unique = extract_unique_demographics(train_data)\n    \n    if unique.shape[0] == 0:\n        print(\"No demographics found; disabling late fusion.\")\n        return 0, None, np.zeros((0,), dtype=np.float32), np.array([], dtype=int)\n    \n    all_nan = np.isnan(unique).all(axis=0)\n    keep_mask = ~all_nan\n    keep_idx = np.where(keep_mask)[0]\n    keep_names = [n for n, k in zip(['age', 'sex', 'hand'], keep_mask) if k]\n    print(\"Keeping demo columns:\", keep_names)\n    \n    kept = unique[:, keep_idx] if keep_idx.size > 0 else np.zeros((unique.shape[0], 0), dtype=np.float32)\n    \n    if kept.shape[1] == 0:\n        print(\"No usable demographic columns; disabling late fusion.\")\n        return 0, None, np.zeros((0,), dtype=np.float32), np.array([], dtype=int)\n    \n    with np.errstate(all='ignore'):\n        col_medians = np.nanmedian(kept, axis=0).astype(np.float32)\n        col_medians[~np.isfinite(col_medians)] = 0.0\n    \n    def impute_cols(arr, meds):\n        out = arr.copy()\n        for j in range(out.shape[1]):\n            mask = ~np.isfinite(out[:, j])\n            out[mask, j] = meds[j]\n        return out\n    \n    kept_imp = impute_cols(kept, col_medians)\n    scaler = SafeStandardScaler().fit(kept_imp)\n    print(f\"Demo scaler fitted on {kept_imp.shape[0]} subjects | dims: {keep_idx.size}\")\n    \n    def transform_batch(demo_tensor):\n        if demo_tensor.numel() == 0 or keep_idx.size == 0:\n            return demo_tensor.to(device=device, dtype=torch.float32)\n        \n        demo_np = demo_tensor.detach().cpu().numpy().astype(np.float32)\n        for j in range(demo_np.shape[1]):\n            mask = ~np.isfinite(demo_np[:, j])\n            demo_np[mask, j] = col_medians[j]\n        \n        demo_np = scaler.transform(demo_np)\n        out = torch.from_numpy(demo_np).to(device=device, dtype=torch.float32)\n        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n        return out\n    \n    return keep_idx.size, transform_batch, col_medians, keep_idx\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Loading training releases...\")\nprint(\"=\"*60)\n\n# *** NOW CALLING load_release_data_lazy ***\ntrain_data_pointers = []\nfor release in TRAIN_RELEASES:\n    train_data_pointers.extend(load_release_data_lazy(release, task=TASK, data_root=DATA_ROOT)) \n\nprint(f\"\\n✓ Total training windows (pointers): {len(train_data_pointers)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Loading validation release...\")\nprint(\"=\"*60)\n\nval_data_pointers = load_release_data_lazy(VAL_RELEASE, TASK, DATA_ROOT)\nprint(f\"✓ Total validation windows (pointers): {len(val_data_pointers)}\")\n\n# Build demographics transform - IMPORTANT: Use pointers for demographics\nprint(\"\\n\" + \"=\"*60)\nprint(\"Building demographic transformations...\")\nprint(\"=\"*60)\ndemodim, transform_demo_batch, demo_medians, keep_idx = build_demo_transform(train_data_pointers)\n\n# Create datasets\nprint(\"\\n\" + \"=\"*60)\nprint(\"Creating PyTorch datasets...\")\nprint(\"=\"*60)\n\nif len(train_data_pointers) == 0:\n    raise ValueError(\"No training window pointers found – check your BIDS structure and data validity!\")\n\ntrain_dataset = EEGWindowsDataset(\n    train_data_pointers,\n    crop_samples=int(CROP_SEC * SFREQ),\n    keep_idx=keep_idx,\n    seed=42,\n    cache_size=5 # Cache the 5 most recently accessed raw files\n)\n\nval_dataset = EEGWindowsDataset(\n    val_data_pointers,\n    crop_samples=int(CROP_SEC * SFREQ),\n    keep_idx=keep_idx,\n    seed=42,\n    cache_size=5 \n)\n\n# DataLoaders\nBATCH_SIZE = 128\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n\n# Infer shapes (must happen after dataset creation)\n# T=200 for 2-second crop\nC, T = 128, int(CROP_SEC * SFREQ) \nprint(f\"\\n✓ Train batches: {len(train_loader)}\")\nprint(f\"✓ Val batches: {len(val_loader)}\")\nprint(f\"\\n✓ Sample EEG shape: ({C}, {T})\")\nprint(f\"✓ Demographic features: {demodim}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA LOADING COMPLETE!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T07:13:29.692760Z","iopub.execute_input":"2025-12-02T07:13:29.693039Z","iopub.status.idle":"2025-12-02T07:18:45.181977Z","shell.execute_reply.started":"2025-12-02T07:13:29.693018Z","shell.execute_reply":"2025-12-02T07:18:45.181286Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nLoading training releases...\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Scanning R6_L100_bdf for windows: 100%|██████████| 135/135 [01:12<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Scanned and generated 32789 window pointers from R6_L100_bdf\n\n✓ Total training windows (pointers): 32789\n\n============================================================\nLoading validation release...\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Scanning R5_L100_bdf for windows: 100%|██████████| 330/330 [04:02<00:00,  1.36it/s]","output_type":"stream"},{"name":"stdout","text":"Scanned and generated 106562 window pointers from R5_L100_bdf\n✓ Total validation windows (pointers): 106562\n\n============================================================\nBuilding demographic transformations...\n============================================================\nKeeping demo columns: ['age', 'sex']\nDemo scaler fitted on 92 subjects | dims: 2\n\n============================================================\nCreating PyTorch datasets...\n============================================================\n\n✓ Train batches: 257\n✓ Val batches: 833\n\n✓ Sample EEG shape: (128, 200)\n✓ Demographic features: 2\n\n============================================================\nDATA LOADING COMPLETE!\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.25):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True\n        )\n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        return attn_output\n\nclass EnhancedEEGNetRegressor(nn.Module):\n    def __init__(self, n_channels: int = 128, n_times: int = 200, n_demographic_features: int = 3, dropout: float = 0.25, F1: int = 16, D: int = 2, num_heads: int = 8):\n        super().__init__()\n        self.n_channels = n_channels\n        self.dropout_rate = dropout\n        self.temporal_conv = nn.Conv2d(1, F1, kernel_size=(1, 51), stride=(1, 1), padding=(0, 25), bias=False)\n        self.bn1 = nn.BatchNorm2d(F1)\n        self.spatial_conv = nn.Conv2d(F1, F1 * D, kernel_size=(n_channels, 1), stride=(1, 1), groups=F1, bias=False)\n        self.bn2 = nn.BatchNorm2d(F1 * D)\n        self.elu = nn.ELU()\n        self.pool1 = nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4))\n        self.dropout1 = nn.Dropout(p=dropout)\n        self.attention = MultiHeadAttention(embed_dim=F1 * D, num_heads=num_heads, dropout=dropout)\n        self.pool2 = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n        self.dropout2 = nn.Dropout(p=dropout)\n        self.eeg_feature_dim = F1 * D * (n_times // 8) # 32 * 25 = 800 for 200 samples\n        \n        self.demographic_encoder = nn.Sequential(\n            nn.Linear(n_demographic_features, 16), nn.ReLU(), nn.Dropout(p=dropout), nn.Linear(16, 32)\n        )\n        fusion_input_dim = self.eeg_feature_dim + 32 if n_demographic_features > 0 else self.eeg_feature_dim\n        self.fusion = nn.Sequential(\n            nn.Linear(fusion_input_dim, 64), nn.ReLU(), nn.Dropout(p=dropout),\n            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(p=dropout)\n        )\n        self.regression_head = nn.Linear(32, 1)\n    \n    def forward(self, eeg: torch.Tensor, demographics: torch.Tensor = None):\n        x = self.temporal_conv(eeg)\n        x = self.bn1(x)\n        x = self.elu(x)\n        x = self.spatial_conv(x)\n        x = self.bn2(x)\n        x = self.elu(x)\n        x = self.pool1(x)\n        x = self.dropout1(x)\n        batch_size = x.shape[0]\n        x = x.squeeze(2).transpose(1, 2)\n        x = self.attention(x)\n        x = x.transpose(1, 2).unsqueeze(2)\n        x = self.pool2(x)\n        x = self.dropout2(x)\n        eeg_features = x.reshape(batch_size, -1)\n        \n        if demographics is not None and demographics.numel() > 0:\n            demo_features = self.demographic_encoder(demographics)\n            combined_features = torch.cat([eeg_features, demo_features], dim=1)\n        else:\n            combined_features = eeg_features\n        \n        fused = self.fusion(combined_features)\n        output = self.regression_head(fused)\n        return output\n\n# Loss and Metric functions (Unchanged)\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    def forward(self, pred, target):\n        return torch.sqrt(self.mse(pred, target) + 1e-8)\n\nclass NRMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    def forward(self, pred, target):\n        rmse = torch.sqrt(self.mse(pred, target) + 1e-8)\n        target_range = target.max() - target.min() + 1e-8\n        return rmse / target_range\n\ndef calculate_metrics(predictions, targets):\n    mse = np.mean((predictions - targets) ** 2)\n    rmse = np.sqrt(mse)\n    target_range = targets.max() - targets.min()\n    nrmse = rmse / (target_range + 1e-8)\n    mae = np.mean(np.abs(predictions - targets))\n    if len(predictions) > 1 and np.std(predictions) > 0 and np.std(targets) > 0:\n        corr, _ = pearsonr(predictions, targets)\n    else:\n        corr = 0.0\n    return {'mse': mse, 'rmse': rmse, 'nrmse': nrmse, 'mae': mae, 'pearson_r': corr}\n\nclass EarlyStoppingCallback:\n    def __init__(self, patience: int = 3, verbose: bool = True):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n    \n    def __call__(self, val_loss: float, model: nn.Module, save_path: str = \"best_model.pt\"):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            torch.save(model.state_dict(), save_path)\n        elif val_loss < self.best_loss * 0.99:\n            self.best_loss = val_loss\n            self.counter = 0\n            torch.save(model.state_dict(), save_path)\n            if self.verbose:\n                print(f\"✓ Validation loss improved to {val_loss:.6f}. Model saved.\")\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"No improvement for {self.counter}/{self.patience} epochs\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.verbose:\n                    print(\"Early stopping triggered!\")\n\ndef train_epoch(model, train_loader, criterion, optimizer, device, grad_clip=1.0, transform_demo_batch=None):\n    model.train()\n    total_loss = 0.0\n    \n    for eeg_batch, target_batch, demo_batch, _, _ in tqdm(train_loader, desc=\"Train\", leave=False):\n        eeg_batch = eeg_batch.unsqueeze(1).to(device)\n        target_batch = target_batch.to(device)\n        \n        if demo_batch.numel() > 0 and transform_demo_batch is not None:\n            demo_batch_transformed = transform_demo_batch(demo_batch)\n        else:\n            demo_batch_transformed = None\n        \n        optimizer.zero_grad()\n        predictions = model(eeg_batch, demo_batch_transformed)\n        loss = criterion(predictions, target_batch)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n\ndef validate_epoch(model, val_loader, criterion, device, transform_demo_batch=None):\n    model.eval()\n    total_loss = 0.0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for eeg_batch, target_batch, demo_batch, _, _ in tqdm(val_loader, desc=\"Val\", leave=False):\n            eeg_batch = eeg_batch.unsqueeze(1).to(device)\n            target_batch = target_batch.to(device)\n            \n            if demo_batch.numel() > 0 and transform_demo_batch is not None:\n                demo_batch_transformed = transform_demo_batch(demo_batch)\n            else:\n                demo_batch_transformed = None\n            \n            predictions = model(eeg_batch, demo_batch_transformed)\n            loss = criterion(predictions, target_batch)\n            \n            total_loss += loss.item()\n            all_predictions.append(predictions.cpu().numpy())\n            all_targets.append(target_batch.cpu().numpy())\n    \n    avg_loss = total_loss / len(val_loader)\n    predictions = np.concatenate(all_predictions, axis=0).flatten()\n    targets = np.concatenate(all_targets, axis=0).flatten()\n    \n    return avg_loss, predictions, targets\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"INITIALIZING MODEL\")\nprint(\"=\"*60)\n\n# C and T are defined based on the configuration (128 channels, 200 crop samples)\nmodel = EnhancedEEGNetRegressor(\n    n_channels=C, \n    n_times=T,     \n    n_demographic_features=demodim,\n    dropout=0.5,\n    F1=16,\n    D=2,\n    num_heads=8\n).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"✓ Model created\")\nprint(f\"✓ Total parameters: {total_params:,}\")\nprint(f\"✓ Trainable parameters: {trainable_params:,}\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\nN_EPOCHS = 100\nLEARNING_RATE = 0.001\nEARLY_STOPPING_PATIENCE = 3\nUSE_NRMSE = True\n\nif USE_NRMSE:\n    criterion = NRMSELoss()\n    print(\"Using nRMSE loss\")\nelse:\n    criterion = RMSELoss()\n    print(\"Using RMSE loss\")\n\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n)\n\nearly_stopping = EarlyStoppingCallback(patience=EARLY_STOPPING_PATIENCE, verbose=True)\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_rmse': [], 'val_mae': [], 'val_pearson_r': []}\n\nbest_model_path = \"/kaggle/working/best_enhanced_eegnet.pt\"\n\nfor epoch in range(N_EPOCHS):\n    print(f\"\\n{'='*60}\")\n    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n    print(f\"{'='*60}\")\n    \n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, transform_demo_batch=transform_demo_batch)\n    history['train_loss'].append(train_loss)\n    print(f\"Train Loss: {train_loss:.6f}\")\n    \n    val_loss, val_preds, val_targets = validate_epoch(model, val_loader, criterion, device, transform_demo_batch=transform_demo_batch)\n    history['val_loss'].append(val_loss)\n    \n    metrics = calculate_metrics(val_preds, val_targets)\n    history['val_rmse'].append(metrics['rmse'])\n    history['val_mae'].append(metrics['mae'])\n    history['val_pearson_r'].append(metrics['pearson_r'])\n    \n    print(f\"Val Loss: {val_loss:.6f}\")\n    print(f\"Val RMSE: {metrics['rmse']:.6f}\")\n    print(f\"Val nRMSE: {metrics['nrmse']:.6f}\")\n    print(f\"Val MAE: {metrics['mae']:.6f}\")\n    print(f\"Val Pearson r: {metrics['pearson_r']:.4f}\")\n    \n    scheduler.step(val_loss)\n    \n    early_stopping(val_loss, model, best_model_path)\n    if early_stopping.early_stop:\n        print(f\"\\n✓ Early stopping at epoch {epoch+1}\")\n        break\n\ntry:\n    model.load_state_dict(torch.load(best_model_path))\n    print(f\"\\n✓ Loaded best model from {best_model_path}\")\nexcept:\n    print(\"\\nWarning: Could not load best model state dict. Using model from last epoch.\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*60)\n\nval_loss, val_preds, val_targets = validate_epoch(model, val_loader, criterion, device, transform_demo_batch=transform_demo_batch)\nfinal_metrics = calculate_metrics(val_preds, val_targets)\n\nprint(f\"Final Val RMSE: {final_metrics['rmse']:.6f}\")\nprint(f\"Final Val MAE: {final_metrics['mae']:.6f}\")\nprint(f\"Final Val Pearson r: {final_metrics['pearson_r']:.4f}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T07:18:45.183372Z","iopub.execute_input":"2025-12-02T07:18:45.183667Z","iopub.status.idle":"2025-12-02T10:36:13.420895Z","shell.execute_reply.started":"2025-12-02T07:18:45.183647Z","shell.execute_reply":"2025-12-02T10:36:13.420057Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nINITIALIZING MODEL\n============================================================\n✓ Model created\n✓ Total parameters: 65,249\n✓ Trainable parameters: 65,249\n============================================================\n\n============================================================\nSTARTING TRAINING\n============================================================\nUsing nRMSE loss\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nEpoch 1/100\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.205441\n","output_type":"stream"},{"name":"stderr","text":"                                                      \r","output_type":"stream"},{"name":"stdout","text":"Val Loss: 41759928.598244\nVal RMSE: 0.740065\nVal nRMSE: 0.185248\nVal MAE: 0.576316\nVal Pearson r: 0.0330\n\n============================================================\nEpoch 2/100\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.165907\n","output_type":"stream"},{"name":"stderr","text":"                                                      \r","output_type":"stream"},{"name":"stdout","text":"Val Loss: 43426882.892121\nVal RMSE: 0.757540\nVal nRMSE: 0.189622\nVal MAE: 0.589513\nVal Pearson r: 0.0159\nNo improvement for 1/3 epochs\n\n============================================================\nEpoch 3/100\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.151031\n","output_type":"stream"},{"name":"stderr","text":"                                                      \r","output_type":"stream"},{"name":"stdout","text":"Val Loss: 44231684.149598\nVal RMSE: 0.757876\nVal nRMSE: 0.189706\nVal MAE: 0.595593\nVal Pearson r: 0.0481\nNo improvement for 2/3 epochs\n\n============================================================\nEpoch 4/100\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.141963\n","output_type":"stream"},{"name":"stderr","text":"                                                      \r","output_type":"stream"},{"name":"stdout","text":"Val Loss: 44336312.464239\nVal RMSE: 0.760737\nVal nRMSE: 0.190422\nVal MAE: 0.595697\nVal Pearson r: 0.0297\nNo improvement for 3/3 epochs\nEarly stopping triggered!\n\n✓ Early stopping at epoch 4\n\n✓ Loaded best model from /kaggle/working/best_enhanced_eegnet.pt\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n\n============================================================\nFINAL EVALUATION\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                      ","output_type":"stream"},{"name":"stdout","text":"Final Val RMSE: 0.740065\nFinal Val MAE: 0.576316\nFinal Val Pearson r: 0.0330\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\naxes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\naxes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training and Validation Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\naxes[0, 1].plot(history['val_rmse'], label='Val RMSE', linewidth=2, color='orange')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('RMSE')\naxes[0, 1].set_title('Validation RMSE')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\naxes[1, 0].plot(history['val_mae'], label='Val MAE', linewidth=2, color='green')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('MAE')\naxes[1, 0].set_title('Validation MAE')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\naxes[1, 1].plot(history['val_pearson_r'], label='Val Pearson r', linewidth=2, color='red')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Pearson r')\naxes[1, 1].set_title('Validation Pearson Correlation')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Training curves saved to training_history.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}