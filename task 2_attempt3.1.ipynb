{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13944918,"sourceType":"datasetVersion","datasetId":8885317},{"sourceId":668742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":506359,"modelId":521151}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mne==1.10.1 mne-bids==0.17.0 --quiet\n!pip install scikit-learn --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:39:26.404507Z","iopub.execute_input":"2025-12-01T14:39:26.405265Z","iopub.status.idle":"2025-12-01T14:39:38.384427Z","shell.execute_reply.started":"2025-12-01T14:39:26.405235Z","shell.execute_reply":"2025-12-01T14:39:38.383644Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\n\ndef get_actual_release_path(data_root, release_name):\n    \"\"\"\n    Returns the path to the release by checking for double nesting.\n    \"\"\"\n    outer = data_root / release_name\n    inner = outer / release_name\n    if inner.exists():\n        return inner\n    return outer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:02:17.134734Z","iopub.execute_input":"2025-12-01T15:02:17.135094Z","iopub.status.idle":"2025-12-01T15:02:17.139747Z","shell.execute_reply.started":"2025-12-01T15:02:17.135061Z","shell.execute_reply":"2025-12-01T15:02:17.138845Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from pathlib import Path\nimport os, math, random\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\n\nimport mne\nfrom mne_bids import BIDSPath, read_raw_bids\nmne.set_log_level('ERROR')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:09:36.056802Z","iopub.execute_input":"2025-12-01T15:09:36.057439Z","iopub.status.idle":"2025-12-01T15:09:36.062283Z","shell.execute_reply.started":"2025-12-01T15:09:36.057413Z","shell.execute_reply":"2025-12-01T15:09:36.061649Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Configuration\nSFREQ = 100  # Sampling rate (Hz)\nWIN_SEC = 4  # Window size (seconds)\nCROP_SEC = 2  # Random crop size (seconds)\nSTRIDE_SEC = 2  # Window stride (seconds)\nTASK = \"contrastChangeDetection\"  # Primary task\n\n# Kaggle paths - UPDATE THIS with your dataset name\nDATA_ROOT = Path(\"/kaggle/input/eeg-dataset/R6_L100_bdf/R6_L100_bdf\")\n\nTRAIN_RELEASES = [\"R6_L100_bdf\"]\nVAL_RELEASE = \"R5_L100_bdf\"\n\nSUB_RM = [\"NDARAC350XUM\", \"NDARAJ689BVN\"] \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\ndef seed_all(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_all(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:19:09.153745Z","iopub.execute_input":"2025-12-01T15:19:09.154156Z","iopub.status.idle":"2025-12-01T15:19:09.161675Z","shell.execute_reply.started":"2025-12-01T15:19:09.154131Z","shell.execute_reply":"2025-12-01T15:19:09.161029Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nCUDA available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"def resolve_double_nested_path(data_root, release):\n    outer = data_root / release\n    inner = outer / release\n    # If inner folder exists and contains participants.tsv, use it\n    if (inner.exists() and (inner / \"participants.tsv\").exists()):\n        return inner\n    elif (outer.exists() and (outer / \"participants.tsv\").exists()):\n        return outer\n    else:\n        # If neither has participants.tsv, return None\n        return None\n\ndef load_participants_data(release_path):\n    \"\"\"Load participants.tsv file with demographics and labels\"\"\"\n    participants_file = release_path / \"participants.tsv\"\n    if not participants_file.exists():\n        print(f\"Warning: {participants_file} not found\")\n        return pd.DataFrame()\n    \n    df = pd.read_csv(participants_file, sep='\\t')\n    return df\n\n\ndef load_eeg_file(subject_path, subject_id, task, run=None):\n    \"\"\"Load a single EEG file using MNE-BIDS\"\"\"\n    try:\n        # root is the BIDS root; for sub-XXX/eeg/*.bdf this is subject_path.parent.parent\n        bids_path = BIDSPath(\n            subject=subject_id,\n            task=task,\n            run=run,\n            datatype='eeg',\n            extension='.bdf',\n            root=subject_path.parent.parent\n        )\n        raw = read_raw_bids(bids_path, verbose=False)\n        raw.load_data()\n        return raw\n    except Exception:\n        # Silently skip files that can't be loaded\n        return None\n\n\ndef create_windows_from_raw(raw, win_samples, stride_samples):\n    \"\"\"Create fixed-length windows from raw EEG data\"\"\"\n    data = raw.get_data()  # (n_channels, n_times)\n    n_channels, n_times = data.shape\n\n    windows = []\n    starts = range(0, n_times - win_samples + 1, stride_samples)\n\n    for start in starts:\n        end = start + win_samples\n        window = data[:, start:end]  # (n_channels, win_samples)\n        windows.append(window)\n\n    return np.array(windows) if windows else np.array([]).reshape(0, n_channels, win_samples)\n\n\ndef _resolve_release_path(data_root, release):\n    \"\"\"\n    Handle double nesting like:\n      /kaggle/input/dataset/R5_L100_bdf/R5_L100_bdf/...\n    Returns the *inner* folder if present, else the outer one.\n    \"\"\"\n    outer = data_root / release\n    inner = outer / release\n    if inner.exists():\n        return inner\n    return outer\n\n\ndef load_release_data(release, task=TASK, data_root=DATA_ROOT):\n    # Resolve actual folder that contains participants.tsv and sub-XXX\n    release_path = _resolve_release_path(data_root, release)\n\n    if not release_path.exists():\n        print(f\"Warning: Release path {release_path} not found\")\n        return []\n\n    # Load demographics/labels\n    participants_df = load_participants_data(release_path)\n\n    if participants_df.empty:\n        print(f\"No participants data found for {release}\")\n        return []\n\n    dataset = []\n\n    # Iterate through subjects\n    for _, row in tqdm(\n        participants_df.iterrows(),\n        total=len(participants_df),\n        desc=f\"Loading {release}\"\n    ):\n        subject_id = row['participant_id'].replace('sub-', '')\n\n        # Skip excluded subjects\n        if subject_id in SUB_RM:\n            continue\n\n        # Extract demographics and label\n        age = row.get('age', np.nan)\n        sex = row.get('sex', np.nan)\n        handedness = row.get('handedness', np.nan)\n        externalizing = row.get('externalizing', np.nan)\n\n        # Validate externalizing score\n        try:\n            externalizing = float(externalizing)\n            if not math.isfinite(externalizing):\n                continue\n        except Exception:\n            continue\n\n        # Process sex encoding\n        sex_str = str(sex).strip().lower()\n        if sex_str in ['female', 'f', '2']:\n            sex_encoded = 1.0\n        elif sex_str in ['male', 'm', '1']:\n            sex_encoded = 0.0\n        else:\n            sex_encoded = np.nan\n\n        # Path to subject directory inside the resolved release folder\n        subject_path = release_path / f\"sub-{subject_id}\"\n\n        # For contrastChangeDetection, try all runs\n        runs = [1, 2, 3] if task == \"contrastChangeDetection\" else [None]\n\n        for run in runs:\n            raw = load_eeg_file(subject_path, subject_id, task, run)\n\n            if raw is None:\n                continue\n\n            # Check valid length\n            if raw.n_times < 4 * SFREQ:\n                continue\n\n            # Check channel count (128 EEG + 1 reference = 129)\n            if len(raw.ch_names) != 129:\n                continue\n\n            # Create windows\n            win_samples = int(WIN_SEC * SFREQ)\n            stride_samples = int(STRIDE_SEC * SFREQ)\n            windows = create_windows_from_raw(raw, win_samples, stride_samples)\n\n            if windows.shape[0] == 0:\n                continue\n\n            # Store each window with metadata\n            for window in windows:\n                dataset.append(\n                    {\n                        \"eeg\": window,  # (129, 400)\n                        \"subject\": subject_id,\n                        \"task\": task,\n                        \"run\": run,\n                        \"age\": age,\n                        \"sex\": sex_encoded,\n                        \"handedness\": handedness,\n                        \"externalizing\": externalizing,\n                    }\n                )\n\n    print(f\"Loaded {len(dataset)} windows from {release}\")\n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:19:10.278872Z","iopub.execute_input":"2025-12-01T15:19:10.279654Z","iopub.status.idle":"2025-12-01T15:19:10.293082Z","shell.execute_reply.started":"2025-12-01T15:19:10.279629Z","shell.execute_reply":"2025-12-01T15:19:10.292221Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"class EEGWindowsDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset for windowed EEG data with demographics\"\"\"\n    \n    def __init__(self, data_list, crop_samples, keep_idx, seed=42):\n        \"\"\"\n        Args:\n            data_list: List of dicts with keys ['eeg', 'subject', 'externalizing', etc.]\n            crop_samples: Number of samples for random crop\n            keep_idx: Indices of demographic features to keep\n            seed: Random seed\n        \"\"\"\n        self.data_list = data_list\n        self.crop_samples = crop_samples\n        self.keep_idx = keep_idx\n        self.rng = random.Random(seed)\n    \n    def __len__(self):\n        return len(self.data_list)\n    \n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        \n        # Get EEG data\n        eeg = item['eeg']  # (n_channels, n_times)\n        \n        # Take first 128 channels if 129 present\n        if eeg.shape[0] == 129:\n            eeg = eeg[:128, :]\n        \n        # Convert to tensor\n        eeg = torch.from_numpy(eeg.copy()).float()\n        C, T = eeg.shape\n        \n        # Random crop\n        if T < self.crop_samples:\n            # Pad if too short\n            pad_amount = self.crop_samples - T\n            eeg = torch.nn.functional.pad(eeg, (0, pad_amount), mode='constant', value=0)\n            start = 0\n            stop = self.crop_samples\n        else:\n            start = self.rng.randint(0, T - self.crop_samples)\n            stop = start + self.crop_samples\n            eeg = eeg[:, start:stop]\n        \n        # Per-window z-score normalization\n        mu = eeg.mean(dim=1, keepdim=True)\n        sd = eeg.std(dim=1, keepdim=True)\n        eeg = (eeg - mu) / (sd + 1e-6)\n        eeg = torch.nan_to_num(eeg, nan=0.0, posinf=0.0, neginf=0.0)\n        eeg = torch.clamp(eeg, min=-1e3, max=1e3)\n        \n        # Get label\n        y = torch.tensor([item['externalizing']], dtype=torch.float32)\n        \n        # Get demographics\n        age = item['age']\n        try:\n            age = float(age) if age is not None and math.isfinite(float(age)) else np.nan\n        except:\n            age = np.nan\n        \n        sex = item['sex']\n        hand = item['handedness']\n        try:\n            hand = float(hand) if hand is not None and math.isfinite(float(hand)) else np.nan\n        except:\n            hand = np.nan\n        \n        # Build demo array\n        full_demo = np.array([age, sex, hand], dtype=np.float32)\n        if len(self.keep_idx) > 0:\n            demo = torch.from_numpy(full_demo[self.keep_idx])\n        else:\n            demo = torch.empty(0, dtype=torch.float32)\n        \n        info = {\n            'subject': item['subject'],\n            'task': item['task'],\n            'run': item['run'],\n        }\n        crop_idx = (start, stop)\n        \n        return eeg, y, demo, crop_idx, info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:19:11.248563Z","iopub.execute_input":"2025-12-01T15:19:11.248834Z","iopub.status.idle":"2025-12-01T15:19:11.258733Z","shell.execute_reply.started":"2025-12-01T15:19:11.248813Z","shell.execute_reply":"2025-12-01T15:19:11.258156Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"\n\ndef extract_unique_demographics(data_list):\n    \"\"\"Extract unique subject demographics\"\"\"\n    seen = {}\n    for item in data_list:\n        sid = item['subject']\n        if sid in seen:\n            continue\n        \n        age = item['age']\n        try:\n            age = float(age) if age is not None and math.isfinite(float(age)) else np.nan\n        except:\n            age = np.nan\n        \n        sex = item['sex']\n        hand = item['handedness']\n        try:\n            hand = float(hand) if hand is not None and math.isfinite(float(hand)) else np.nan\n        except:\n            hand = np.nan\n        \n        seen[sid] = [age, sex, hand]\n    \n    arr = np.array(list(seen.values()), dtype=np.float32) if seen else np.zeros((0, 3), dtype=np.float32)\n    return arr\n\nclass SafeStandardScaler(StandardScaler):\n    \"\"\"Robust scaler with NaN/Inf safeguards\"\"\"\n    def fit(self, X, y=None):\n        super().fit(X, y)\n        if hasattr(self, \"scale_\"):\n            bad = ~np.isfinite(self.scale_) | (self.scale_ == 0)\n            self.scale_[bad] = 1.0\n        if hasattr(self, \"var_\"):\n            self.var_[~np.isfinite(self.var_)] = 0.0\n        if hasattr(self, \"mean_\"):\n            self.mean_[~np.isfinite(self.mean_)] = 0.0\n        return self\n\ndef build_demo_transform(train_data):\n    \"\"\"Build demographic transformation pipeline\"\"\"\n    unique = extract_unique_demographics(train_data)\n    \n    if unique.shape[0] == 0:\n        print(\"No demographics found; disabling late fusion.\")\n        return 0, None, np.zeros((0,), dtype=np.float32), np.array([], dtype=int)\n    \n    # Detect all-NaN columns\n    all_nan = np.isnan(unique).all(axis=0)\n    keep_mask = ~all_nan\n    keep_idx = np.where(keep_mask)[0]\n    keep_names = [n for n, k in zip(['age', 'sex', 'hand'], keep_mask) if k]\n    print(\"Keeping demo columns:\", keep_names)\n    \n    kept = unique[:, keep_idx] if keep_idx.size > 0 else np.zeros((unique.shape[0], 0), dtype=np.float32)\n    \n    if kept.shape[1] == 0:\n        print(\"No usable demographic columns; disabling late fusion.\")\n        return 0, None, np.zeros((0,), dtype=np.float32), np.array([], dtype=int)\n    \n    # Compute column medians for imputation\n    with np.errstate(all='ignore'):\n        col_medians = np.nanmedian(kept, axis=0).astype(np.float32)\n        col_medians[~np.isfinite(col_medians)] = 0.0\n    \n    # Impute and fit scaler\n    def impute_cols(arr, meds):\n        out = arr.copy()\n        for j in range(out.shape[1]):\n            mask = ~np.isfinite(out[:, j])\n            out[mask, j] = meds[j]\n        return out\n    \n    kept_imp = impute_cols(kept, col_medians)\n    scaler = SafeStandardScaler().fit(kept_imp)\n    print(f\"Demo scaler fitted on {kept_imp.shape[0]} subjects | dims: {keep_idx.size}\")\n    \n    def transform_batch(demo_tensor):\n        if demo_tensor.numel() == 0 or keep_idx.size == 0:\n            return demo_tensor.to(device=device, dtype=torch.float32)\n        \n        demo_np = demo_tensor.detach().cpu().numpy().astype(np.float32)\n        for j in range(demo_np.shape[1]):\n            mask = ~np.isfinite(demo_np[:, j])\n            demo_np[mask, j] = col_medians[j]\n        \n        demo_np = scaler.transform(demo_np)\n        out = torch.from_numpy(demo_np).to(device=device, dtype=torch.float32)\n        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n        return out\n    \n    return keep_idx.size, transform_batch, col_medians, keep_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:19:11.919059Z","iopub.execute_input":"2025-12-01T15:19:11.919368Z","iopub.status.idle":"2025-12-01T15:19:11.931829Z","shell.execute_reply.started":"2025-12-01T15:19:11.919349Z","shell.execute_reply":"2025-12-01T15:19:11.931008Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"Loading training releases...\")\nprint(\"=\"*60)\n\nDATA_ROOT = Path(\"/kaggle/input/eeg-dataset\")\nTRAIN_RELEASES = [\"R6_L100_bdf\", \"R5_L100_bdf\"]  # List the releases you need\n\ntrain_data = []\nfor release in TRAIN_RELEASES:\n    resolved = resolve_double_nested_path(DATA_ROOT, release)\n    if resolved is None:\n        print(f\"Could not find a valid release folder for {release}\")\n        continue\n    print(f\"Using release path for {release}: {resolved}\")\n    train_data.extend(load_release_data(release, task=TASK, data_root=resolved.parent))\n\n\n\nprint(f\"\\n✓ Total training windows: {len(train_data)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Loading validation release...\")\nprint(\"=\"*60)\n\nval_data = load_release_data(VAL_RELEASE, TASK, DATA_ROOT)\nprint(f\"✓ Total validation windows: {len(val_data)}\")\n\n# Build demographics transform\nprint(\"\\n\" + \"=\"*60)\nprint(\"Building demographic transformations...\")\nprint(\"=\"*60)\ndemodim, transform_demo_batch, demo_medians, keep_idx = build_demo_transform(train_data)\n\n# Create datasets\nprint(\"\\n\" + \"=\"*60)\nprint(\"Creating PyTorch datasets...\")\nprint(\"=\"*60)\n\ntrain_dataset = EEGWindowsDataset(\n    train_data,\n    crop_samples=int(CROP_SEC * SFREQ),\n    keep_idx=keep_idx,\n    seed=42\n)\n\nval_dataset = EEGWindowsDataset(\n    val_data,\n    crop_samples=int(CROP_SEC * SFREQ),\n    keep_idx=keep_idx,\n    seed=42\n)\nif len(train_data) == 0:\n    raise ValueError(\"No training windows found – check your folder nesting, names, and participants.tsv presence!\")\n\n# DataLoaders\nBATCH_SIZE = 32\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"\\n✓ Train batches: {len(train_loader)}\")\nprint(f\"✓ Val batches: {len(val_loader)}\")\n\n# Infer shapes\nsample_X, sample_y, sample_demo, _, _ = train_dataset[0]\nC, T = sample_X.shape\nprint(f\"\\n✓ Sample EEG shape: ({C}, {T})\")\nprint(f\"✓ Demographic features: {demodim}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA LOADING COMPLETE!\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:19:13.828353Z","iopub.execute_input":"2025-12-01T15:19:13.828874Z","iopub.status.idle":"2025-12-01T15:19:14.524774Z","shell.execute_reply.started":"2025-12-01T15:19:13.828847Z","shell.execute_reply":"2025-12-01T15:19:14.523845Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nLoading training releases...\n============================================================\nUsing release path for R6_L100_bdf: /kaggle/input/eeg-dataset/R6_L100_bdf/R6_L100_bdf\n","output_type":"stream"},{"name":"stderr","text":"Loading R6_L100_bdf: 100%|██████████| 135/135 [00:00<00:00, 1245.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded 0 windows from R6_L100_bdf\nUsing release path for R5_L100_bdf: /kaggle/input/eeg-dataset/R5_L100_bdf/R5_L100_bdf\n","output_type":"stream"},{"name":"stderr","text":"Loading R5_L100_bdf: 100%|██████████| 330/330 [00:00<00:00, 1198.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded 0 windows from R5_L100_bdf\n\n✓ Total training windows: 0\n\n============================================================\nLoading validation release...\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Loading R5_L100_bdf: 100%|██████████| 330/330 [00:00<00:00, 1246.91it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded 0 windows from R5_L100_bdf\n✓ Total validation windows: 0\n\n============================================================\nBuilding demographic transformations...\n============================================================\nNo demographics found; disabling late fusion.\n\n============================================================\nCreating PyTorch datasets...\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4190481908.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No training windows found – check your folder nesting, names, and participants.tsv presence!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No training windows found – check your folder nesting, names, and participants.tsv presence!"],"ename":"ValueError","evalue":"No training windows found – check your folder nesting, names, and participants.tsv presence!","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Self-Attention for EEG channel relationships\"\"\"\n    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.3):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n    \n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        return attn_output\n\nclass EnhancedEEGNetRegressor(nn.Module):\n    \"\"\"\n    Enhanced EEGNet for continuous externalizing score regression.\n    \n    Key modifications from standard EEGNet:\n    - Regression output (1 continuous value)\n    - Batch normalization after conv blocks\n    - Multi-head self-attention\n    - Increased dropout (0.5)\n    - Late fusion with demographic features\n    \"\"\"\n    def __init__(\n        self,\n        n_channels: int = 128,\n        n_times: int = 200,\n        n_demographic_features: int = 3,\n        dropout: float = 0.5,\n        F1: int = 16,\n        D: int = 2,\n        num_heads: int = 8,\n    ):\n        super().__init__()\n        self.n_channels = n_channels\n        self.dropout_rate = dropout\n        \n        # EEG processing branch\n        self.temporal_conv = nn.Conv2d(\n            1, F1, kernel_size=(1, 51), stride=(1, 1),\n            padding=(0, 25), bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(F1)\n        \n        self.spatial_conv = nn.Conv2d(\n            F1, F1 * D, kernel_size=(n_channels, 1), stride=(1, 1),\n            groups=F1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(F1 * D)\n        \n        self.elu = nn.ELU()\n        self.pool1 = nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4))\n        self.dropout1 = nn.Dropout(p=dropout)\n        \n        # Multi-head attention\n        self.attention = MultiHeadAttention(\n            embed_dim=F1 * D,\n            num_heads=num_heads,\n            dropout=dropout\n        )\n        \n        self.pool2 = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n        self.dropout2 = nn.Dropout(p=dropout)\n        \n        # Calculate EEG feature dimension\n        self.eeg_feature_dim = F1 * D * 25  # 32 * 25 = 800\n        \n        # Demographic fusion branch\n        self.demographic_encoder = nn.Sequential(\n            nn.Linear(n_demographic_features, 16),\n            nn.ReLU(),\n            nn.Dropout(p=dropout),\n            nn.Linear(16, 32)\n        )\n        \n        # Fusion and regression head\n        fusion_input_dim = self.eeg_feature_dim + 32\n        self.fusion = nn.Sequential(\n            nn.Linear(fusion_input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(p=dropout),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(p=dropout)\n        )\n        \n        self.regression_head = nn.Linear(32, 1)\n    \n    def forward(self, eeg: torch.Tensor, demographics: torch.Tensor = None):\n        # EEG branch\n        x = self.temporal_conv(eeg)\n        x = self.bn1(x)\n        x = self.elu(x)\n        \n        x = self.spatial_conv(x)\n        x = self.bn2(x)\n        x = self.elu(x)\n        \n        x = self.pool1(x)\n        x = self.dropout1(x)\n        \n        # Reshape for attention\n        batch_size = x.shape[0]\n        x = x.squeeze(2)\n        x = x.transpose(1, 2)\n        \n        # Apply attention\n        x = self.attention(x)\n        x = x.transpose(1, 2)\n        x = x.unsqueeze(2)\n        \n        x = self.pool2(x)\n        x = self.dropout2(x)\n        \n        # Flatten\n        eeg_features = x.view(batch_size, -1)\n        \n        # Demographic branch\n        if demographics is not None and demographics.numel() > 0:\n            demo_features = self.demographic_encoder(demographics)\n            combined_features = torch.cat([eeg_features, demo_features], dim=1)\n        else:\n            combined_features = eeg_features\n        \n        # Fusion and regression\n        fused = self.fusion(combined_features)\n        output = self.regression_head(fused)\n        \n        return output\n\nclass RMSELoss(nn.Module):\n    \"\"\"Root Mean Square Error Loss\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    \n    def forward(self, pred, target):\n        return torch.sqrt(self.mse(pred, target) + 1e-8)\n\nclass NRMSELoss(nn.Module):\n    \"\"\"Normalized RMSE Loss\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    \n    def forward(self, pred, target):\n        rmse = torch.sqrt(self.mse(pred, target) + 1e-8)\n        target_range = target.max() - target.min() + 1e-8\n        return rmse / target_range\n\ndef calculate_metrics(predictions, targets):\n    \"\"\"Calculate regression metrics\"\"\"\n    mse = np.mean((predictions - targets) ** 2)\n    rmse = np.sqrt(mse)\n    nrmse = rmse / (targets.max() - targets.min() + 1e-8)\n    mae = np.mean(np.abs(predictions - targets))\n    \n    # Pearson correlation\n    if len(predictions) > 1:\n        corr, _ = pearsonr(predictions, targets)\n    else:\n        corr = 0.0\n    \n    return {\n        'mse': mse,\n        'rmse': rmse,\n        'nrmse': nrmse,\n        'mae': mae,\n        'pearson_r': corr\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:02:19.493480Z","iopub.status.idle":"2025-12-01T15:02:19.493720Z","shell.execute_reply.started":"2025-12-01T15:02:19.493610Z","shell.execute_reply":"2025-12-01T15:02:19.493620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStoppingCallback:\n    \"\"\"Early stopping with model checkpointing\"\"\"\n    def __init__(self, patience: int = 10, verbose: bool = True):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n    \n    def __call__(self, val_loss: float, model: nn.Module, save_path: str = \"best_model.pt\"):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            torch.save(model.state_dict(), save_path)\n        elif val_loss < self.best_loss * 0.99:\n            self.best_loss = val_loss\n            self.counter = 0\n            torch.save(model.state_dict(), save_path)\n            if self.verbose:\n                print(f\"✓ Validation loss improved to {val_loss:.6f}. Model saved.\")\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"No improvement for {self.counter}/{self.patience} epochs\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.verbose:\n                    print(\"Early stopping triggered!\")\n\ndef train_epoch(model, train_loader, criterion, optimizer, device, grad_clip=1.0):\n    \"\"\"Single training epoch\"\"\"\n    model.train()\n    total_loss = 0.0\n    \n    for eeg_batch, target_batch, demo_batch, _, _ in tqdm(train_loader, desc=\"Train\", leave=False):\n        # Add channel dimension: (B, C, T) -> (B, 1, C, T)\n        eeg_batch = eeg_batch.unsqueeze(1).to(device)\n        target_batch = target_batch.to(device)\n        demo_batch = demo_batch.to(device) if demo_batch.numel() > 0 else None\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        predictions = model(eeg_batch, demo_batch)\n        loss = criterion(predictions, target_batch)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(train_loader)\n\ndef validate_epoch(model, val_loader, criterion, device):\n    \"\"\"Validation epoch\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for eeg_batch, target_batch, demo_batch, _, _ in tqdm(val_loader, desc=\"Val\", leave=False):\n            eeg_batch = eeg_batch.unsqueeze(1).to(device)\n            target_batch = target_batch.to(device)\n            demo_batch = demo_batch.to(device) if demo_batch.numel() > 0 else None\n            \n            predictions = model(eeg_batch, demo_batch)\n            loss = criterion(predictions, target_batch)\n            \n            total_loss += loss.item()\n            all_predictions.append(predictions.cpu().numpy())\n            all_targets.append(target_batch.cpu().numpy())\n    \n    avg_loss = total_loss / len(val_loader)\n    predictions = np.concatenate(all_predictions, axis=0).flatten()\n    targets = np.concatenate(all_targets, axis=0).flatten()\n    \n    return avg_loss, predictions, targets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"INITIALIZING MODEL\")\nprint(\"=\"*60)\n\nmodel = EnhancedEEGNetRegressor(\n    n_channels=128,\n    n_times=200,\n    n_demographic_features=demodim,\n    dropout=0.5,\n    F1=16,\n    D=2,\n    num_heads=8\n).to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"✓ Model created\")\nprint(f\"✓ Total parameters: {total_params:,}\")\nprint(f\"✓ Trainable parameters: {trainable_params:,}\")\nprint(\"=\"*60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\n# Hyperparameters\nN_EPOCHS = 100\nLEARNING_RATE = 0.001\nEARLY_STOPPING_PATIENCE = 15\nUSE_NRMSE = True\n\n# Loss function\nif USE_NRMSE:\n    criterion = NRMSELoss()\n    print(\"Using nRMSE loss\")\nelse:\n    criterion = RMSELoss()\n    print(\"Using RMSE loss\")\n\n# Optimizer and scheduler\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n)\n\n# Early stopping\nearly_stopping = EarlyStoppingCallback(patience=EARLY_STOPPING_PATIENCE, verbose=True)\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_rmse': [],\n    'val_mae': [],\n    'val_pearson_r': []\n}\n\nbest_model_path = \"/kaggle/working/best_enhanced_eegnet.pt\"\n\n# Training loop\nfor epoch in range(N_EPOCHS):\n    print(f\"\\n{'='*60}\")\n    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n    print(f\"{'='*60}\")\n    \n    # Train\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    history['train_loss'].append(train_loss)\n    print(f\"Train Loss: {train_loss:.6f}\")\n    \n    # Validate\n    val_loss, val_preds, val_targets = validate_epoch(model, val_loader, criterion, device)\n    history['val_loss'].append(val_loss)\n    \n    # Calculate metrics\n    metrics = calculate_metrics(val_preds, val_targets)\n    history['val_rmse'].append(metrics['rmse'])\n    history['val_mae'].append(metrics['mae'])\n    history['val_pearson_r'].append(metrics['pearson_r'])\n    \n    print(f\"Val Loss: {val_loss:.6f}\")\n    print(f\"Val RMSE: {metrics['rmse']:.6f}\")\n    print(f\"Val nRMSE: {metrics['nrmse']:.6f}\")\n    print(f\"Val MAE: {metrics['mae']:.6f}\")\n    print(f\"Val Pearson r: {metrics['pearson_r']:.4f}\")\n    \n    # Learning rate scheduling\n    scheduler.step(val_loss)\n    \n    # Early stopping\n    early_stopping(val_loss, model, best_model_path)\n    if early_stopping.early_stop:\n        print(f\"\\n✓ Early stopping at epoch {epoch+1}\")\n        break\n\n# Load best model\nmodel.load_state_dict(torch.load(best_model_path))\nprint(f\"\\n✓ Loaded best model from {best_model_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Loss curves\naxes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\naxes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training and Validation Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# RMSE\naxes[0, 1].plot(history['val_rmse'], label='Val RMSE', linewidth=2, color='orange')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('RMSE')\naxes[0, 1].set_title('Validation RMSE')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# MAE\naxes[1, 0].plot(history['val_mae'], label='Val MAE', linewidth=2, color='green')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('MAE')\naxes[1, 0].set_title('Validation MAE')\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Pearson correlation\naxes[1, 1].plot(history['val_pearson_r'], label='Val Pearson r', linewidth=2, color='red')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Pearson r')\naxes[1, 1].set_title('Validation Pearson Correlation')\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Training curves saved to training_history.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*60)\n\n# Final validation metrics\nval_loss, val_preds, val_targets = validate_epoch(model, val_loader, criterion, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exploration_eeg2025.py\n\nimport os\nfrom pathlib import Path\nimport json\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\n# Root directory for your uploaded dataset (change as needed)\nDATA_ROOT = Path(\"/kaggle/input/eeg-dataset/R5_L100_bdf/R5_L100_bdf\")  \n# e.g. \"/kaggle/input/eegchallenge2025/R11_L100_bdf\"\n\ndef scan_dataset(root_dir: Path):\n    \"\"\"\n    Walk the dataset directory, and collect statistics:\n      - number of subjects\n      - tasks per subject\n      - EEG file counts\n      - behavioral / metadata files\n    \"\"\"\n    subjects = sorted([p for p in root_dir.iterdir() if p.is_dir() and p.name.startswith(\"sub-\")])\n    print(f\"Found {len(subjects)} subjects\")\n\n    summary = {}\n    for sub in subjects:\n        summary[sub.name] = {}\n        # tasks: likely subfolders under eeg/ or beh/ — inspect\n        eeg_dir = sub / \"eeg\"\n        beh_dir = sub / \"beh\"\n        if eeg_dir.exists():\n            eeg_files = list(eeg_dir.glob(\".bdf\")) + list(eeg_dir.glob(\".set\")) + list(eeg_dir.glob(\"*.npy\"))\n            summary[sub.name][\"n_eeg_files\"] = len(eeg_files)\n            summary[sub.name][\"eeg_files\"] = [f.name for f in eeg_files]\n        else:\n            summary[sub.name][\"n_eeg_files\"] = 0\n\n        if beh_dir.exists():\n            beh_files = [f.name for f in beh_dir.iterdir() if f.is_file()]\n            summary[sub.name][\"beh_files\"] = beh_files\n        else:\n            summary[sub.name][\"beh_files\"] = []\n\n    return summary\n\ndef load_one_subject_sample(sub_dir: Path):\n    \"\"\"\n    Try to load one EEG + metadata from a subject folder to inspect content.\n    \"\"\"\n    eeg_dir = sub_dir / \"eeg\"\n    beh_dir = sub_dir / \"beh\"\n    # pick first EEG file\n    eeg_files = list(eeg_dir.glob(\".npy\")) + list(eeg_dir.glob(\".bdf\")) + list(eeg_dir.glob(\"*.set\"))\n    if not eeg_files:\n        print(f\"No EEG files for {sub_dir.name}\")\n        return\n\n    eeg_file = eeg_files[0]\n    print(\"Loading EEG:\", eeg_file)\n    # If it's .npy, load via numpy\n    if eeg_file.suffix == \".npy\":\n        eeg = np.load(eeg_file)\n    else:\n        print(\"Non-npy EEG format:\", eeg_file.suffix,\n              \"- you need MNE or other library to load BDF/SET\")\n        return\n    print(\" EEG shape:\", eeg.shape, \" dtype:\", eeg.dtype)\n\n    # Load behavior/metadata if exists\n    beh_files = list(beh_dir.glob(\".\"))\n    if beh_files:\n        print(\"Behavior / metadata files:\", [f.name for f in beh_files])\n        for f in beh_files:\n            if f.suffix.lower() in [\".json\", \".csv\", \".tsv\"]:\n                try:\n                    if f.suffix.lower() == \".json\":\n                        with open(f, \"r\") as fp:\n                            beh = json.load(fp)\n                        print(\" Example keys:\", list(beh.keys())[:10])\n                    else:\n                        df = pd.read_csv(f)\n                        print(\" Dataframe columns:\", df.columns.tolist())\n                except Exception as e:\n                    print(\" Error loading \", f, e)\n    else:\n        print(\"No behavioral/metadata files.\")\n\ndef main():\n    summary = scan_dataset(DATA_ROOT)\n    pprint.pprint(summary)\n\n    # load sample from first subject\n    subjects = sorted([p for p in DATA_ROOT.iterdir() if p.is_dir() and p.name.startswith(\"sub-\")])\n    if subjects:\n        load_one_subject_sample(subjects[0])\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T15:15:05.756828Z","iopub.execute_input":"2025-12-01T15:15:05.757124Z","iopub.status.idle":"2025-12-01T15:15:14.160528Z","shell.execute_reply.started":"2025-12-01T15:15:05.757106Z","shell.execute_reply":"2025-12-01T15:15:14.159876Z"}},"outputs":[{"name":"stdout","text":"Found 330 subjects\n{'sub-NDARAC350XUM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAC857HDB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAH304ED7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAH793FBF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAJ689BVN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAK738BGC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAM848GTE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAP522AFK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAP785CTE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAT358XM9': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAU708TL8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAV187GJ5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARAX358NB5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBC240LNC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBE091BGD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBE103DHM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBF851NH6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBG188RA5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBH228RDW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBH992ARB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBJ674TVU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBK638HLZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBL214YLX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBM433VER': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARBN365EV3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCA740UC8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCD661GL0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCG159AAP': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCG438NML': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCH868WVX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCM717HCJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCR582GKJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCU633GCZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCU736GZ1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCU744XWL': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCV606ZZ5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARCV981XUY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDA573XGG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDB224XUD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDC843HHM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDD854GF8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDE877RFH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDF374LGN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDH086ZKK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDL305BT8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDR804MFE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDT499DWP': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDU853XZ6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDV245WJG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDY776AKH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARDZ528DLZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREC078VFT': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREC480KFA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARED628HMQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREE675XRY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREF624KJN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREF848YWD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREG590BNY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREK395BM3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREK575JNM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREL410HHK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREN417DJE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREV342ABE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREX589EXG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDAREY897LB1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFA165XBV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFA737TG6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFB322DRA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFB969EMV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFE299MWU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFE526NK3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFG568PXZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFG713PLR': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFG851ZNZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFG943GVZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFG953RRK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFH018DLG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFM211KDJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFM229HHA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFP512MEK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFR095UJK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFU395UBW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFU444HEF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARFU916UZY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGA048BD8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGD425PTN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGF445UFB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGF543PM2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGH842VFY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGK672URV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGL085RTW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGL936KTV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGM196JMM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGN044FE4': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGR125APK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGR354WBH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGR893PUA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGV948BFZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGX760NYV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGY542TJ5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGZ207NNA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARGZ361JJ9': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHC453LC1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHD931FNA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHE899FJV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHG065YHQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHG594GKH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHN212TT6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHR140GMB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHT019ER6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHT829KKB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHU910KZC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHW933UVJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHX934KJ5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHX963YU1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARHY099CUL': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJA166LHK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJA262HTY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJA788CH7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJB366HYJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJF517HC8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJG298YVA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJH065GKG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJH367WKY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJH492TVW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJH763NPD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJJ767FBH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJK651XB0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJK842BCN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJM296MDW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJP948FJV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJR579FW7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJT588ZAX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJU903AZH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJX458DVE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJX505XD6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJX939UCQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARJZ888UX6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKA946MJ1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKD064EVC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKH444AGK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKK000PNQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKK392ZX2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKM301DY0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKU649DB7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKV353MTD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKV807EMJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKX665ZD3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARKZ634RVX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLA788EWU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLB162XP2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLD978JVJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLE091XAZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLE417RA4': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLF032LXH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLM196YRG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLM313FVX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLU939YX4': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLW774VY1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLY114PDC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLY687YDQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARLZ701LJ3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMC473UHD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARME100GK1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMF668NU5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMG451PJA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMH231UEP': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMH559LKK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMK416UUQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMK525TGP': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMP977RHZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMR306ZKG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMR631KR5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMR684AH2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMU589LP6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMV981GZ7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMY254BGN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMY294VNE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMY456WML': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMY873XG5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMZ189EFC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARMZ366UY8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNB905CFU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARND697FLK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNE406XC3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNF873FCV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNH200DA6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNM708YTF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNR459MUJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNT245TP0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNU770PM5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNY023FWG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNY419GR2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNY875DR2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARNZ610KZY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPB293EPC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPB619PHV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPD029JVJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPD568LHV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPD855ARC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPE551CK7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPE752VYE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPF319WV2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPF459CME': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPF640ZRG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPF937BDQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPK237YHD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPL194XDX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPL215MTH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPL596YTD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPP622WV4': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPP726XBF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPT890YBR': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPU329MDJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPV009TYX': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPX838GCD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARPZ344ZGE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRB901DWV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRH285LR3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRH344TL7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRM073JKA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRM122UPB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRU290EZV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRU399NYG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRU729XRU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRU979UBW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRV505ND6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRW951BYF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRX800KW8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRX897XV1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRY538NE1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARRZ927VC3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTC865YNZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTD925CTP': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTF776EYR': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTF781TM8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTG396FGE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTK357VHL': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTK682LJE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTM501BT2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARTW685ENR': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUB962AXN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUC771VM5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUD776KFT': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUD871XPD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUE398GGV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUG492VF0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUJ744NEU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUJ894ZXG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUL675RXW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUM797TFA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUV041YCY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUV578DU2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUX086TVT': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUX534HZQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUX885LMW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUY529FNK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUY549PGQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUY730ANT': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUZ368BT4': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARUZ601TW4': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVA930UA3': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVB902GA5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVC195NLH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVD552PRC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVF159VP9': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVG132NF6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVG761NR2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVH153RE7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVL964HH1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVN363NNQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVN920VHJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVP285RXV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVU883NDE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARVV473XTY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWA433WYZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWD585PRZ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWE898TG8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWH779MZ2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWJ498CZY': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWK983JFE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWM656UWL': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWN885MGD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWP815VUP': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWR888KKT': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWT715BMK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWV568MEU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWV794VM2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWW820WZN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARWZ903DD2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXB692PXC': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXD710RLN': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXG670HFE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXH393TMM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXK009VGV': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXK986EK6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXL023GFG': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXL697DA6': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXN140AZE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXN764AC7': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXP557DLJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXP717YMH': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXR346UT5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARXV034WE0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYA898BRE': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYC466ER1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYE221LZB': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYE379GGT': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYE521RDA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYF338WW1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYG665JUD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYG988PUQ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYH697TPA': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYJ583ZMJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYJ735XPK': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYM334BZ5': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYX008UCU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYX806FL1': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYY022HBW': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARYZ693DE2': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZA511FYF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZF170TZ0': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZG861CXJ': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZH366BF8': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZH521MMD': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZJ587UZU': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZM903TNL': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZN088VER': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZW472CCF': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0},\n 'sub-NDARZZ740MLM': {'beh_files': [], 'eeg_files': [], 'n_eeg_files': 0}}\nNo EEG files for sub-NDARAC350XUM\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}